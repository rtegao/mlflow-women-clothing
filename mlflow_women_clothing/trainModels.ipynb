{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from TextPreProcessing.preProcessing import TextPreProcessing"
   ]
  },
  {
   "source": [
    "# Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Womens Clothing E-Commerce Reviews.csv')\n",
    "# df = df.head(10)"
   ]
  },
  {
   "source": [
    "# Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16440, 12)\n(3523, 12)\n(3523, 12)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df,test_size = 0.3,random_state=7)\n",
    "test, val = train_test_split(test,test_size=0.5,random_state=7)\n",
    "\n",
    "train.reset_index(inplace = True)\n",
    "val.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "source": [
    "del df"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "# Text Preprocessing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_train = TextPreProcessing(sentences= train['Review Text'])\n",
    "pre_processing_val = TextPreProcessing (sentences= val['Review Text'])\n",
    "pre_processing_test = TextPreProcessing (sentences= test['Review Text'])\n",
    "\n",
    "pre_processing_train = pd.Series(pre_processing_train.preprocessing())\n",
    "pre_processing_val = pd.Series(pre_processing_val.preprocessing())\n",
    "pre_processing_test = pd.Series(pre_processing_test.preprocessing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'features': pre_processing_train,\n",
    "                        'label': train['Recommended IND']})\n",
    "\n",
    "df_val = pd.DataFrame({'features': pre_processing_val,\n",
    "                        'label': val['Recommended IND']})\n",
    "\n",
    "df_test = pd.DataFrame({'features': pre_processing_test,\n",
    "                        'label': test['Recommended IND']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train['features'] != np.nan) & (df_train['features'] != '- E M P T Y -')]\n",
    "\n",
    "df_val = df_val[(df_val['features'] != np.nan) & (df_val['features'] != '- E M P T Y -')]\n",
    "\n",
    "df_test = df_test[(df_test['features'] != np.nan) & (df_test['features'] != '- E M P T Y -')]\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pre_processing_train, pre_processing_val, pre_processing_test\n",
    "del train, test, val"
   ]
  },
  {
   "source": [
    "# TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "bow_train = tfidf_vectorizer.fit_transform(df_train['features'])\n",
    "bow_val = tfidf_vectorizer.transform(df_val['features'])\n",
    "bow_test = tfidf_vectorizer.transform(df_test['features'])"
   ]
  },
  {
   "source": [
    "# Training with MLflow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"mlflow server \\\n",
    "#     --backend-store-uri sqlite:///mlflow.db \\\n",
    "#     --default-artifact-root ./artifacts \\\n",
    "#     --host 127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"Bag of Words\"\n",
    "try:\n",
    "    mlflow.create_experiment(experiment)\n",
    "    mlflow.set_experiment(experiment)\n",
    "except:\n",
    "    mlflow.set_experiment(experiment)"
   ]
  },
  {
   "source": [
    "# Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'NaiveBayes'\n",
    "with mlflow.start_run(run_name= run_name):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(bow_train,df_train['label'])\n",
    "\n",
    "    train_pred = model.predict(bow_train)\n",
    "    val_pred = model.predict(bow_val)\n",
    "    test_pred = model.predict(bow_test)\n",
    "\n",
    "    mlflow.log_metric('f1 score - train', f1_score(df_train['label'], train_pred))\n",
    "    mlflow.log_metric('precision - train', precision_score(df_train['label'], train_pred))\n",
    "    mlflow.log_metric('recall - train', recall_score(df_train['label'], train_pred))\n",
    "    mlflow.log_metric('accuracy - train', accuracy_score(df_train['label'], train_pred))\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(model,\"NaiveBayes\")"
   ]
  },
  {
   "source": [
    "# SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'SVM_Classifier'\n",
    "parans = {\n",
    "    'C':[0.1,1,10,100],\n",
    "    'kernel':['linear', 'rbf', 'poly'],\n",
    "    'degree':[2]\n",
    "    }\n",
    "with mlflow.start_run(run_name= run_name):\n",
    "    model = GridSearchCV(estimator=svm.SVC(), \n",
    "                    param_grid=parans, \n",
    "                    scoring=['f1',\n",
    "                            'precision',\n",
    "                            'recall',\n",
    "                            'accuracy'],\n",
    "                    refit='f1')\n",
    "\n",
    "    model.fit(bow_train,df_train['label'])\n",
    "\n",
    "\n",
    "    train_pred = model.best_estimator_.predict(bow_train)\n",
    "    val_pred = model.best_estimator_.predict(bow_val)\n",
    "    test_pred = model.best_estimator_.predict(bow_test)\n",
    "\n",
    "    metrics_train = {\n",
    "        \"f1-train\": f1_score(df_train['label'], train_pred),\n",
    "        \"precision-train\": precision_score(df_train['label'], train_pred),\n",
    "        \"recall-train\": recall_score(df_train['label'], train_pred),\n",
    "        \"accuracy-train\": accuracy_score(df_train['label'], train_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_train)\n",
    "\n",
    "\n",
    "    metrics_val = {\n",
    "        \"f1-val\": f1_score(df_val['label'], val_pred),\n",
    "        \"precision-val\": precision_score(df_val['label'], val_pred),\n",
    "        \"recall-val\": recall_score(df_val['label'], val_pred),\n",
    "        \"accuracy-val\": accuracy_score(df_val['label'], val_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_val)\n",
    "\n",
    "    metrics_test = {\n",
    "        \"f1-test\": f1_score(df_test['label'], test_pred),\n",
    "        \"precision-test\": precision_score(df_test['label'], test_pred),\n",
    "        \"recall-test\": recall_score(df_test['label'], test_pred),\n",
    "        \"accuracy-test\": accuracy_score(df_test['label'], test_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_test)    \n",
    "    mlflow.log_params(model.best_params_)"
   ]
  },
  {
   "source": [
    "# XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_name = 'XGBoost_Classifier'\n",
    "parans = {\n",
    "    'learning_rate' : [0.01],\n",
    "    'max_depth' : [5,10], \n",
    "    'n_estimators' : [200,500],\n",
    "    'objective' : ['binary:logistic'],\n",
    "    'eval_metric':['mlogloss'],\n",
    "    'seed' : [42],\n",
    "    'reg_lambda' : [5,8],\n",
    "    'reg_alpha' : [2,4],\n",
    "    'gamma' : [3,5],\n",
    "    'subsample': [0.4,0.8],\n",
    "    }\n",
    "with mlflow.start_run(run_name= run_name):\n",
    "    model = GridSearchCV(estimator=xgb.XGBClassifier(), \n",
    "                    param_grid=parans, \n",
    "                    scoring=['f1',\n",
    "                            'precision',\n",
    "                            'recall',\n",
    "                            'accuracy'],\n",
    "                    refit='f1')\n",
    "\n",
    "    model.fit(bow_train,df_train['label'])\n",
    "\n",
    "\n",
    "    train_pred = model.best_estimator_.predict(bow_train)\n",
    "    val_pred = model.best_estimator_.predict(bow_val)\n",
    "    test_pred = model.best_estimator_.predict(bow_test)\n",
    "\n",
    "    metrics_train = {\n",
    "        \"f1-train\": f1_score(df_train['label'], train_pred),\n",
    "        \"precision-train\": precision_score(df_train['label'], train_pred),\n",
    "        \"recall-train\": recall_score(df_train['label'], train_pred),\n",
    "        \"accuracy-train\": accuracy_score(df_train['label'], train_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_train)\n",
    "\n",
    "\n",
    "    metrics_val = {\n",
    "        \"f1-val\": f1_score(df_val['label'], val_pred),\n",
    "        \"precision-val\": precision_score(df_val['label'], val_pred),\n",
    "        \"recall-val\": recall_score(df_val['label'], val_pred),\n",
    "        \"accuracy-val\": accuracy_score(df_val['label'], val_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_val)\n",
    "\n",
    "    metrics_test = {\n",
    "        \"f1-test\": f1_score(df_test['label'], test_pred),\n",
    "        \"precision-test\": precision_score(df_test['label'], test_pred),\n",
    "        \"recall-test\": recall_score(df_test['label'], test_pred),\n",
    "        \"accuracy-test\": accuracy_score(df_test['label'], test_pred)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics_test)    \n",
    "    mlflow.log_params(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}